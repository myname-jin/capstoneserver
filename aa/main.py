from fastapi import FastAPI, UploadFile, File, HTTPException, status, BackgroundTasks
from fastapi.responses import JSONResponse, FileResponse
import uvicorn
from contextlib import asynccontextmanager
from pathlib import Path
import time as timer 
import uuid 
import os 
from fastapi.middleware.cors import CORSMiddleware 

# ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë“ˆ ì„í¬íŠ¸
from utils.helpers import setup_temp_dirs, create_session_dirs, save_upload_file, cleanup_dirs
# â—ï¸ [ìˆ˜ì •] â—ï¸: audio_extractor ì„í¬íŠ¸
from processing.video_analyzer import extract_all_frames, extract_audio
from processing.face_analyzer import setup_face_landmarker, analyze_image
# â—ï¸ [ìˆ˜ì •] â—ï¸: audio_analyzer (ë¡œì»¬ Whisper) ì„í¬íŠ¸
from processing.audio_analyzer import transcribe_audio_with_timestamps, load_local_whisper_model
from processing.ai_scorer import is_openai_configured # (GPT ì±„ì ì€ ì—¬ì „íˆ ì œì™¸)

# --- ì„¤ì • ---
FRAME_RATE = 5
BASE_DIR = Path(__file__).resolve().parent

job_status = {}

# 
# â—ï¸ [ìƒˆ í•¨ìˆ˜] â—ï¸: ìŒì„±/ì‹œì„  ë°ì´í„°ë¥¼ í•©ì¹˜ëŠ” í•µì‹¬ ë¡œì§
def align_data(vision_data: list, audio_segments: list) -> list:
    """
    ë¬¸ì¥(audio_segments)ë³„ë¡œ í•´ë‹¹ ì‹œê°„ëŒ€ì˜ í‰ê·  ì‹œì„ /í‘œì •(vision_data)ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    aligned_results = []
    
    valid_vision_data = [frame for frame in vision_data if "error" not in frame]
    if not valid_vision_data:
        return []

    for segment in audio_segments:
        start_time = segment['start']
        end_time = segment['end']
        text = segment['text']
        
        frames_in_segment = [
            frame for frame in valid_vision_data 
            if frame['time'] >= start_time and frame['time'] <= end_time
        ]

        if not frames_in_segment:
            avg_vision = {"error": "ì–¼êµ´ ë¯¸ê²€ì¶œ"}
        else:
            avg_vision = {
                "smile": round(sum(f['smile'] for f in frames_in_segment) / len(frames_in_segment), 3),
                "frown": round(sum(f['frown'] for f in frames_in_segment) / len(frames_in_segment), 3),
                "brow_up": round(sum(f['brow_up'] for f in frames_in_segment) / len(frames_in_segment), 3),
                "brow_down": round(sum(f['brow_down'] for f in frames_in_segment) / len(frames_in_segment), 3),
                "jaw_open": round(sum(f['jaw_open'] for f in frames_in_segment) / len(frames_in_segment), 3),
                "mouth_open": round(sum(f['mouth_open'] for f in frames_in_segment) / len(frames_in_segment), 3),
                "squint": round(sum(f['squint'] for f in frames_in_segment) / len(frames_in_segment), 3),
                "gaze_h": round(sum(f['gaze_h'] for f in frames_in_segment) / len(frames_in_segment), 3),
                "gaze_v": round(sum(f['gaze_v'] for f in frames_in_segment) / len(frames_in_segment), 3),
            }

        aligned_results.append({
            "start": start_time,
            "end": end_time,
            "text": text,
            "vision_avg": avg_vision
        })
        
    return aligned_results


# 
# â—ï¸ [ìˆ˜ì •] â—ï¸: ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ë¡œì§ ì „ì²´ ë³€ê²½ (5ë‹¨ê³„ë¡œ)
def run_analysis_task(job_id: str, video_path: Path, frame_dir: Path, video_dir: Path):
    all_vision_results = []
    audio_path = frame_dir / "audio.wav" 
    
    try:
        # 1. ì˜¤ë””ì˜¤ ì¶”ì¶œ
        job_status[job_id] = {"status": "Analyzing", "message": "1/5: ì˜¤ë””ì˜¤ íŠ¸ë™ ì¶”ì¶œ ì¤‘..."}
        extract_audio(video_path, audio_path)
        
        # 2. í”„ë ˆì„ ì¶”ì¶œ
        job_status[job_id] = {"status": "Analyzing", "message": "2/5: ë¹„ë””ì˜¤ í”„ë ˆì„ ì¶”ì¶œ ì¤‘..."}
        frame_paths = extract_all_frames(video_path, frame_dir, FRAME_RATE)
        
        if not frame_paths:
            raise Exception("ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        total_frames = len(frame_paths)
        
        # 3. ê° í”„ë ˆì„ ë¶„ì„ (MediaPipe)
        job_status[job_id] = {"status": "Analyzing", "message": f"3/5: ì–¼êµ´ ë°ì´í„° ë¶„ì„ ì¤‘... (0/{total_frames})"}
        print(f"   > [3/5] ëª¨ë“  í”„ë ˆì„ ë¶„ì„ ì‹œì‘ (Job: {job_id})...")
        for i, path in enumerate(frame_paths):
            data = analyze_image(str(path))
            data["time"] = i / FRAME_RATE
            all_vision_results.append(data)
            
            if i % 20 == 0 or i == total_frames - 1:
                job_status[job_id] = {
                    "status": "Analyzing", 
                    "message": f"3/5: ì–¼êµ´ ë°ì´í„° ë¶„ì„ ì¤‘...",
                    "progress": i + 1,
                    "total": total_frames
                }
                print(f"     ... {i+1}/{total_frames} í”„ë ˆì„ ì²˜ë¦¬ ì¤‘ (Job: {job_id})")
        
        print(f"   > [3/5] âœ… í”„ë ˆì„ ë¶„ì„ ì™„ë£Œ (Job: {job_id}).")
        
        # 4. ìŒì„± ì¸ì‹ (ë¡œì»¬ Whisper)
        job_status[job_id] = {"status": "Analyzing", "message": "4/5: â—ï¸ë¡œì»¬ ìŒì„± ì¸ì‹ ì‹¤í–‰ ì¤‘... (ì‹œê°„ ì†Œìš”)â—ï¸"}
        audio_segments, whisper_error = transcribe_audio_with_timestamps(str(audio_path))
        
        if whisper_error:
            print(f"   > [4/5] â—ï¸ ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {whisper_error}")
            audio_segments = []
            ai_report_message = f"## ğŸ¤– ë¡œì»¬ ìŒì„±ì¸ì‹ ì˜¤ë¥˜\n\n**ì˜¤ë¥˜:** {whisper_error}\n\nì‹œì„ /í‘œì • ë¶„ì„ ë°ì´í„°ëŠ” ì •ìƒì ìœ¼ë¡œ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            ai_report_message = "## ğŸ¤– ìŒì„± ì¸ì‹ ì™„ë£Œ (ë¡œì»¬)\n\nAI ì±„ì  ê¸°ëŠ¥ì€ í˜„ì¬ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. \n\nìŒì„± ë° ì‹œì„ /í‘œì • ë°ì´í„° ì¶”ì¶œì€ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
            print(f"   > [4/5] âœ… ìŒì„± ì¸ì‹ ì™„ë£Œ (Job: {job_id}).")

        # 5. ë°ì´í„° ì •ë ¬ (Alignment)
        job_status[job_id] = {"status": "Analyzing", "message": "5/5: ìŒì„±/ì‹œì„  ë°ì´í„° ì •ë ¬ ì¤‘..."}
        aligned_data = align_data(all_vision_results, audio_segments)
        print("   > [5/5] âœ… ë°ì´í„° ì •ë ¬ ì™„ë£Œ.")

        # AI ì±„ì ì€ ì—¬ì „íˆ ê±´ë„ˆëœ€
        ai_result = {"ai_feedback": ai_report_message} 
        
        final_result = {
            "ai_assessment": ai_result,
            "analysis_summary": {
                "total_frames_processed": len(all_vision_results),
                "duration_analyzed_sec": len(all_vision_results) / FRAME_RATE,
                "face_detected_frames": len([f for f in all_vision_results if "error" not in f]),
            },
            "raw_data": all_vision_results, # â—ï¸ UI í˜¸í™˜ì„±ì„ ìœ„í•´ 'raw_data' í‚¤ ì‚¬ìš©
            "aligned_transcript_data": aligned_data
        }
        
        job_status[job_id] = {"status": "Complete", "result": final_result}
        print(f"\nâœ…âœ…âœ… [ì‘ì—… ì™„ë£Œ] (Job: {job_id})")

    except Exception as e:
        print(f"\nâŒâŒâŒ [ì‘ì—… ì‹¤íŒ¨] (Job: {job_id})")
        print(f"ì˜¤ë¥˜ ë‚´ìš©: {e}")
        job_status[job_id] = {"status": "Error", "message": str(e)}
    
    finally:
        cleanup_dirs(video_dir, frame_dir)


# 
# â—ï¸ [ìˆ˜ì •] â—ï¸: lifespan í•¨ìˆ˜ë¥¼ ìˆ˜ì •í•˜ì—¬ ë¡œì»¬ Whisper ëª¨ë¸ì„ ë¯¸ë¦¬ ë¡œë“œí•©ë‹ˆë‹¤.
@asynccontextmanager
async def lifespan(app: FastAPI):
    print("="*50)
    print("ì„œë²„ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("API ë¬¸ì„œëŠ” http://127.0.0.1:8000/docs ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
    print(f"â—ï¸UI ì ‘ì†: http://127.0.0.1:8000 â—ï¸")
    print("="*50)
    setup_temp_dirs()
    try:
        # MediaPipe ì–¼êµ´ ëª¨ë¸ ë¡œë“œ
        setup_face_landmarker()
        # â—ï¸ [ìˆ˜ì •] â—ï¸: ë¡œì»¬ Whisper ëª¨ë¸ ë¡œë“œ
        load_local_whisper_model()
        
        if not is_openai_configured():
            print("="*50)
            print("âš ï¸  ê²½ê³ : OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. (AI ì±„ì  ê¸°ëŠ¥ì€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤)")
            print("="*50)
    except Exception as e:
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨! {e}")
    yield
    print("="*50)
    print("ì„œë²„ê°€ ì¢…ë£Œë©ë‹ˆë‹¤.")
    print("="*50)

app = FastAPI(lifespan=lifespan)

# CORS ë¯¸ë“¤ì›¨ì–´ (ëª¨ë“  ì ‘ì† í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/", include_in_schema=False)
async def read_index():
    html_file_path = os.path.join(BASE_DIR, "index.html")
    if not os.path.exists(html_file_path):
        raise HTTPException(status_code=404, detail="index.html íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return FileResponse(html_file_path)

@app.get("/health", summary="ì„œë²„ ë° AI ëª¨ë¸ ìƒíƒœ í™•ì¸")
def health_check():
    try:
        model = setup_face_landmarker()
        model_loaded = (model is not None)
        openai_ready = is_openai_configured()
        if not model_loaded:
            raise Exception("MediaPipe ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return {
            "ok": True, 
            "message": "ì„œë²„ ë° AI ëª¨ë¸ ì •ìƒ",
            "modelLoaded": model_loaded,
            "openaiConfigured": openai_ready
        }
    except Exception as e:
        return JSONResponse(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            content={"ok": False, "error": f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}", "modelLoaded": False}
        )

@app.post("/upload", summary="ë¹„ë””ì˜¤ ë¶„ì„ ì‘ì—… ì‹œì‘")
def upload_and_analyze_video(background_tasks: BackgroundTasks, videoFile: UploadFile = File(...)):
    
    video_dir, frame_dir = create_session_dirs()
    safe_filename = videoFile.filename or "uploaded_video"
    video_path = Path(os.path.join(video_dir, safe_filename))
    
    try:
        print(f"\n[ì‘ì—… ì ‘ìˆ˜] íŒŒì¼: {videoFile.filename} (Type: {videoFile.content_type})")
        save_upload_file(videoFile, video_path)
        
        job_id = str(uuid.uuid4())
        job_status[job_id] = {"status": "Pending", "message": "0/5: ì‘ì—… ëŒ€ê¸° ì¤‘..."} # 
# â—ï¸ [ìˆ˜ì •] â—ï¸: 5ë‹¨ê³„ë¡œ ë³€ê²½
        
        background_tasks.add_task(run_analysis_task, job_id, video_path, frame_dir, video_dir)
        
        print(f"   > Job ID ë°œê¸‰: {job_id}")
        
        return {"job_id": job_id}

    except Exception as e:
        print(f"âŒâŒâŒ [ì—…ë¡œë“œ ì‹¤íŒ¨] ì˜¤ë¥˜: {e}")
        cleanup_dirs(video_dir, frame_dir)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )

@app.get("/status/{job_id}", summary="ì‘ì—… ì§„í–‰ ìƒíƒœ í™•ì¸")
def get_status(job_id: str):
    status = job_status.get(job_id)
    
    if not status:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="ì‘ì—… IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    if status["status"] == "Complete" or status["status"] == "Error":
        return job_status.pop(job_id)
    
    return status

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="127.0.0.1",
        port=8000,
        reload=True
    )
    # .\venv\Scripts\activate    python main.py  http://127.0.0.1:8000