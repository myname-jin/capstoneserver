from pathlib import Path
import time as timer 

# ëª¨ë“  ì²˜ë¦¬ ëª¨ë“ˆì„ ì—¬ê¸°ì„œ ì„í¬íŠ¸
from processing.video_analyzer import extract_all_frames, extract_audio
from processing.face_analyzer import analyze_image
from processing.audio_analyzer import transcribe_audio_with_timestamps, analyze_prosody_for_segments
from processing.ai_scorer import get_ai_score, is_gemini_configured # â­ï¸ [ìˆ˜ì •] Geminië¡œ ë³€ê²½
from processing.data_combiner import align_data
from utils.helpers import cleanup_dirs

FRAME_RATE = 5
job_status = {} # ì‘ì—… ìƒíƒœë¥¼ main.py ëŒ€ì‹  ì—¬ê¸°ì„œ ê´€ë¦¬

def run_analysis_task(job_id: str, video_path: Path, frame_dir: Path, video_dir: Path):
    """
    [â—ï¸ main.pyì—ì„œ ì´ë™ë¨ â—ï¸]
    ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ëŠ” ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ì…ë‹ˆë‹¤.
    (ì´ 6ë‹¨ê³„ë¡œ êµ¬ì„±)
    """
    all_vision_results = []
    audio_path = frame_dir / "audio.wav" 
    
    try:
        # 1. ì˜¤ë””ì˜¤ ì¶”ì¶œ
        job_status[job_id] = {"status": "Analyzing", "message": "1/6: ì˜¤ë””ì˜¤ íŠ¸ë™ ì¶”ì¶œ ì¤‘..."}
        extract_audio(video_path, audio_path)
        
        # 2. í”„ë ˆì„ ì¶”ì¶œ
        job_status[job_id] = {"status": "Analyzing", "message": "2/6: ë¹„ë””ì˜¤ í”„ë ˆì„ ì¶”ì¶œ ì¤‘..."}
        frame_paths = extract_all_frames(video_path, frame_dir, FRAME_RATE)
        
        if not frame_paths:
            raise Exception("ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        total_frames = len(frame_paths)
        
        # 3. ê° í”„ë ˆì„ ë¶„ì„ (MediaPipe)
        job_status[job_id] = {"status": "Analyzing", "message": f"3/6: ì–¼êµ´ ë°ì´í„° ë¶„ì„ ì¤‘... (0/{total_frames})"}
        print(f"   > [3/6] ëª¨ë“  í”„ë ˆì„ ë¶„ì„ ì‹œì‘ (Job: {job_id})...")
        for i, path in enumerate(frame_paths):
            data = analyze_image(str(path))
            data["time"] = i / FRAME_RATE
            all_vision_results.append(data)
            
            if i % 20 == 0 or i == total_frames - 1:
                job_status[job_id] = {
                    "status": "Analyzing", 
                    "message": f"3/6: ì–¼êµ´ ë°ì´í„° ë¶„ì„ ì¤‘...",
                    "progress": i + 1,
                    "total": total_frames
                }
        print(f"   > [3/6] âœ… í”„ë ˆì„ ë¶„ì„ ì™„ë£Œ (Job: {job_id}).")
        
        # 4. ìŒì„± ì¸ì‹ (ë¡œì»¬ Whisper)
        job_status[job_id] = {"status": "Analyzing", "message": "4/6: â—ï¸ë¡œì»¬ ìŒì„± ì¸ì‹ ì‹¤í–‰ ì¤‘... (ì‹œê°„ ì†Œìš”)â—ï¸"}
        audio_segments, whisper_error = transcribe_audio_with_timestamps(str(audio_path))
        
        ai_report_message = "" # AI ì±„ì  ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ ë©”ì‹œì§€
        if whisper_error:
            print(f"   > [4/6] â—ï¸ ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {whisper_error}")
            audio_segments = []
            ai_report_message = f"## ğŸ¤– ë¡œì»¬ ìŒì„±ì¸ì‹ ì˜¤ë¥˜\n\n**ì˜¤ë¥˜:** {whisper_error}\n\nì‹œì„ /í‘œì • ë¶„ì„ ë°ì´í„°ëŠ” ì •ìƒì ìœ¼ë¡œ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            print(f"   > [4/6] âœ… ìŒì„± ì¸ì‹ ì™„ë£Œ (Job: {job_id}).")

        # 5. ìŒì„± ìš´ìœ¨ ë¶„ì„ (Praat)
        job_status[job_id] = {"status": "Analyzing", "message": "5/6: â—ï¸ìŒì„± ìš´ìœ¨(ëª©ì†Œë¦¬ ë–¨ë¦¼) ë¶„ì„ ì¤‘...â—ï¸"}
        audio_segments = analyze_prosody_for_segments(audio_path, audio_segments)

        # 6. ë°ì´í„° ì •ë ¬ ë° AI ì±„ì 
        job_status[job_id] = {"status": "Analyzing", "message": "6/6: ë°ì´í„° ì •ë ¬ ë° AI ì±„ì  ì¤‘..."}
        
        # 6-1. ì •ë ¬
        aligned_data = align_data(all_vision_results, audio_segments)
        
        # 6-2. AI ì±„ì 
        if is_gemini_configured(): # â­ï¸ [ìˆ˜ì •] Geminië¡œ ë³€ê²½
            ai_result = get_ai_score(aligned_data)
        else:
            # WhisperëŠ” ì„±ê³µí–ˆìœ¼ë‚˜ Gemini í‚¤ê°€ ì—†ëŠ” ê²½ìš°
            if not whisper_error:
                ai_report_message = "## ğŸ¤– ìŒì„±/í‘œì •/ìš´ìœ¨ ë¶„ì„ ì™„ë£Œ\n\nGemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ **AI ìë™ ì±„ì  ê¸°ëŠ¥ì€ ë¹„í™œì„±í™”**ë˜ì—ˆìŠµë‹ˆë‹¤. \n\nëŒ€ë³¸, ì‹œì„ /í‘œì •, ëª©ì†Œë¦¬ ë–¨ë¦¼ ë°ì´í„° ì¶”ì¶œì€ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤." # â­ï¸ [ìˆ˜ì •]
            
            ai_result = {"ai_feedback": ai_report_message} # whisper_errorê°€ ìˆì„ ê²½ìš° í•´ë‹¹ ë©”ì‹œì§€ ì‚¬ìš©
        
        print("   > [6/6] âœ… ë°ì´í„° ì •ë ¬ ë° AI ì±„ì  ì™„ë£Œ.")

        final_result = {
            "ai_assessment": ai_result,
            "analysis_summary": {
                "total_frames_processed": len(all_vision_results),
                "duration_analyzed_sec": len(all_vision_results) / FRAME_RATE,
                "face_detected_frames": len([f for f in all_vision_results if "error" not in f]),
            },
            "raw_data": all_vision_results,
            "aligned_transcript_data": aligned_data
        }
        
        job_status[job_id] = {"status": "Complete", "result": final_result}
        print(f"\nâœ…âœ…âœ… [ì‘ì—… ì™„ë£Œ] (Job: {job_id})")

    except Exception as e:
        print(f"\nâŒâŒâŒ [ì‘ì—… ì‹¤íŒ¨] (Job: {job_id})")
        print(f"ì˜¤ë¥˜ ë‚´ìš©: {e}")
        job_status[job_id] = {"status": "Error", "message": str(e)}
    
    finally:
        # ë¶„ì„ì´ ì„±ê³µí•˜ë“  ì‹¤íŒ¨í•˜ë“  ì„ì‹œ íŒŒì¼ ì •ë¦¬
        cleanup_dirs(video_dir, frame_dir)